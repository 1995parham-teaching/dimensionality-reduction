{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b52fcad4-17d1-40b3-9b2a-608943763810",
   "metadata": {},
   "source": [
    "# Inroduction\n",
    "Many ML problems involve thousands or even milions of features which maje training extremely slow and maybe harder to find a good solution --> curse of dimensionality\n",
    "\n",
    "There are two main approaches: \n",
    "1. projection \n",
    "2. manifold learning\n",
    "\n",
    "## Heads up\n",
    "Reducing dimensionality does cause some information loss (compressing an image to JPEG can degrade its quality). It speeds up training but performs slightly worse (in rare cases you may filter noise and unnecessary data), it also makes your pipeline a bit more complex --> first try the original data\n",
    "\n",
    "## Applications\n",
    "1. speed up\n",
    "2. data visualization --> detecting patterns and clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d219042-cce7-40f2-9135-f9590203f9e8",
   "metadata": {},
   "source": [
    "# The Curse of Dimensionality\n",
    "If you pick a random point in a unit square, it will have only about 0.4% chance of being located less than 0.001 from a border (it is very unlikely that a random point will be \"extreme\" along any dimenstion. In a 10,000 dimensional unit hypercube, this probablity is greater than 99.999999%\n",
    "\n",
    "![](img/photo_2023-04-03_17-47-43.jpg)\n",
    "\n",
    "If you pick two points randomly in a unit square the distance between them will be on avrage roghly 0.52. If you pick two random points in a 3D unit cube, the average distance will be roughly 0.66. What about two points picked randomly in a 1,000,000-dimensional unit hypercube? The average distance, will be about 408.25. \n",
    "\n",
    "This is counterintuitive: how can two points be so far apart when they both lie within the same unit hypercube? Well, there’s just plenty of space in high dimensions. As a result, high-dimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other. In short, the more dimensions the training set has, the greater the risk of overfitting it.\n",
    "\n",
    "In theory, one solution to the curse of dimensionality could be to increase the size of the training set to reach a sufficient density of training instances. In practice, the number of training instances required to reach a given density grows exponentially with the number of dimensions. With just 100 features—significantly fewer than in the MNIST problem all ranging from 0 to 1, you would need more training instances than atoms in the observable universe in order for training instances to be within 0.1 of each other on average, assuming they were spread out uniformly across all dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a828987e-427a-4821-824a-8772d49720c0",
   "metadata": {},
   "source": [
    "# Projection\n",
    "Training instances are not spread out uniformly across all dimenstions\n",
    "1. Many features are almost constant\n",
    "2. features are highly correlated\n",
    "Like MNIST dataset.\n",
    "\n",
    "Lie withing or close to a much lower dimensional subspaces\n",
    "![](img/photo_2023-04-03_18-16-02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e27207-75b8-4025-9d29-af780e763d50",
   "metadata": {},
   "source": [
    "# Manifold Learning\n",
    "![](img/photo_2023-04-03_18-27-23.jpg)\n",
    "\n",
    "![](img/photo_2023-04-03_18-27-27.jpg)\n",
    "\n",
    "Squashing by projecting onto a plane (left) versus unrolling the Swiss roll (right)\n",
    "\n",
    "## Heads up\n",
    "\n",
    "![](img/photo_2023-04-03_18-44-40.jpg)\n",
    "The dicision boundary may not always be simpler with lower dimentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfda50ca-fbe6-4981-826f-be3457c267ad",
   "metadata": {},
   "source": [
    "# Principal component analysis (PCA)\n",
    "\n",
    "## Preserving the Variance\n",
    "![](img/photo_2023-04-03_19-09-02.jpg)\n",
    "select the axis that preserves the maximum amount of variance, as it will most likely lose less information (+the axis that minimizes the mean squared distance between the original dataset and its projection onto that axis+)\n",
    "\n",
    "## Principal Components\n",
    "1. Zero-centered unit vector in the direction of the PC\n",
    "2. They are orthogonal to each other\n",
    "+Since two opposing unit vectors lie on the same axis, the direction of the unit vectors returned by PCA is not stable+\n",
    "\n",
    "## How to find them?\n",
    "Standard matrix factorization technique called singular value decomposition (SVD) decomposes the training set matrix $X$ into the matrix multiplication of three matrices $U \\sum V^T$ where $V$ contains the unit vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7bb2dfc4-839b-4e6c-bb4a-cd05321c479b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1:  [-0.7769242   0.16085852  0.60869806]\n",
      "c2:  [ 0.22765677 -0.82961563  0.509814  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the number of data points\n",
    "n = 10\n",
    "\n",
    "# Generate random data for the x, y, and z coordinates\n",
    "x = np.random.rand(n)\n",
    "y = np.random.rand(n)\n",
    "z = np.random.rand(n)\n",
    "\n",
    "# Combine the x, y, and z coordinates into a single dataset\n",
    "data = np.column_stack((x, y, z))\n",
    "\n",
    "data_centered = data - data.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(data_centered)\n",
    "c1 = Vt[0]\n",
    "c2 = Vt[1]\n",
    "\n",
    "print(\"c1: \", c1)\n",
    "print(\"c2: \", c2)\n",
    "\n",
    "# Heads up\n",
    "# PCA assumes that the dataset is centered around the origin, Scikit-Learn's PCA classes take care of centering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee73a8bb-f8ac-4142-a1fc-08f5f52cecfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.33872797 0.80449451 0.65101911]\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0121de97-5d18-491d-92df-74ce5f26914b",
   "metadata": {},
   "source": [
    "## Projecting Down to d  Dimensions \n",
    "Project onto the first d principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f4ebca0-9643-4443-8c7d-0df54eab2f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52198892, -0.06160485],\n",
       "       [-0.02403791,  0.22465357],\n",
       "       [-0.64316296, -0.21995502],\n",
       "       [-0.35694905,  0.30980858],\n",
       "       [ 0.36558658,  0.3628524 ],\n",
       "       [-0.36707219,  0.1160826 ],\n",
       "       [ 0.37629299, -0.2077425 ],\n",
       "       [ 0.11914735, -0.50292153],\n",
       "       [ 0.51310426, -0.07032475],\n",
       "       [ 0.53907985,  0.04915149]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2 = Vt[:2].T\n",
    "W2D = data_centered @ W2\n",
    "W2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e71f77d-db59-409b-9913-c1c932e6611e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52198892,  0.06160485],\n",
       "       [ 0.02403791, -0.22465357],\n",
       "       [ 0.64316296,  0.21995502],\n",
       "       [ 0.35694905, -0.30980858],\n",
       "       [-0.36558658, -0.3628524 ],\n",
       "       [ 0.36707219, -0.1160826 ],\n",
       "       [-0.37629299,  0.2077425 ],\n",
       "       [-0.11914735,  0.50292153],\n",
       "       [-0.51310426,  0.07032475],\n",
       "       [-0.53907985, -0.04915149]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(data)\n",
    "X2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65ae92a2-973b-4d5b-87ec-1f600d7c66c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.7769242  -0.16085852 -0.60869806]\n",
      " [-0.22765677  0.82961563 -0.509814  ]]\n"
     ]
    }
   ],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85df4d29-eb13-424d-a0de-309be7eb35d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62593388 0.22604242]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2eee45-caa4-46a1-8008-b5d9bc7c5895",
   "metadata": {},
   "source": [
    "## Choosing the right number of dimensions\n",
    "Choose the number of dimensioins that add up to a sufficiently large portion of the variance--say, 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db79639b-8cae-49d3-af21-038701e72097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import fetch_openml\n",
    "\n",
    "# mnist = fetch_openml('mnist_784', as_frame=False)\n",
    "\n",
    "# X_train, y_train = mnist.data[:60_000], mnist.target[:60_000]\n",
    "# X_test, y_test = mnist.data[60_000:], mnist.target[60_000:]\n",
    "\n",
    "# pca = PCA()\n",
    "# pca.fit(X_train)\n",
    "\n",
    "# cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "# d = np.argmax(cumsum >= 0.95) + 1 # d equals 154\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0aac32d-ca99-4a3c-9636-eb0312365fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52198892,  0.06160485,  0.22322649],\n",
       "       [ 0.02403791, -0.22465357, -0.05645224],\n",
       "       [ 0.64316296,  0.21995502, -0.05357744],\n",
       "       [ 0.35694905, -0.30980858,  0.22095859],\n",
       "       [-0.36558658, -0.3628524 , -0.27990281],\n",
       "       [ 0.36707219, -0.1160826 , -0.24994704],\n",
       "       [-0.37629299,  0.2077425 ,  0.29645003],\n",
       "       [-0.11914735,  0.50292153, -0.27262856],\n",
       "       [-0.51310426,  0.07032475,  0.06027031],\n",
       "       [-0.53907985, -0.04915149,  0.11160267]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "data_reduced = pca.fit_transform(data)\n",
    "data_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "084c1709-27fc-4cd3-88d5-5788867b2d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9353a681-dfed-4cb6-b9ed-322ebab232a9",
   "metadata": {},
   "source": [
    "![](img/photo_2023-04-03_20-49-18.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ffd0e-4496-4b79-8775-3b80111835cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized Search CV\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# clf = make_pipeline(PCA(random_state=42),\n",
    "#  RandomForestClassifier(random_state=42))\n",
    "# param_distrib = {\n",
    "#  \"pca__n_components\": np.arange(10, 80),\n",
    "#  \"randomforestclassifier__n_estimators\": np.arange(50, 500)\n",
    "# }\n",
    "# rnd_search = RandomizedSearchCV(clf, param_distrib, n_iter=10, cv=3,\n",
    "#  random_state=42)\n",
    "# rnd_search.fit(X_train[:1000], y_train[:1000])\n",
    "# print(rnd_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcfec8b-341d-4218-a0d7-13321795586a",
   "metadata": {},
   "source": [
    "# PCA for Compression\n",
    "1. apply PCA to the MNIST\n",
    "2. 154 features instead of 784\n",
    "3. less than 20% of its original size, we only lost 5% of its variance\n",
    "4. possible to decompress (inverse transformation), this won't give you back the original data\n",
    "5. likely be close to the original data\n",
    "6. mean squared distance between original data and the reconstructed data is called the reconstruction error\n",
    "\n",
    "$X_{recovered} = X_{d-proj} * W_d^T$\n",
    "\n",
    "$W_d$ is orthogonal so $W_d^{-1} = W_d^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f658782-0cc2-4e7e-93eb-b113684a2596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92313538, 0.63253196, 0.33211065],\n",
       "       [0.4372643 , 0.32561384, 0.6111265 ],\n",
       "       [0.81874687, 0.59641357, 0.00934627],\n",
       "       [0.87813544, 0.34973709, 0.62054314],\n",
       "       [0.03485351, 0.15416603, 0.78290402],\n",
       "       [0.56547886, 0.25705189, 0.22934031],\n",
       "       [0.23495094, 0.93741611, 0.84890472],\n",
       "       [0.03348875, 0.83667301, 0.19593551],\n",
       "       [0.0213071 , 0.71914335, 0.85865872],\n",
       "       [0.05845736, 0.65164781, 0.96658705]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_recovered = pca.inverse_transform(data_reduced)\n",
    "data_recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14ae4195-2b6a-4743-b01b-f7a0f5f75a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92313538, 0.63253196, 0.33211065],\n",
       "       [0.4372643 , 0.32561384, 0.6111265 ],\n",
       "       [0.81874687, 0.59641357, 0.00934627],\n",
       "       [0.87813544, 0.34973709, 0.62054314],\n",
       "       [0.03485351, 0.15416603, 0.78290402],\n",
       "       [0.56547886, 0.25705189, 0.22934031],\n",
       "       [0.23495094, 0.93741611, 0.84890472],\n",
       "       [0.03348875, 0.83667301, 0.19593551],\n",
       "       [0.0213071 , 0.71914335, 0.85865872],\n",
       "       [0.05845736, 0.65164781, 0.96658705]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb84d49-4f56-4ee9-9a9d-7f1bd6dd2d5d",
   "metadata": {},
   "source": [
    "![](img/photo_2023-04-03_21-03-34.jpg) \n",
    "MNIST compression that preserves 95% of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3327f646-d57c-479c-8042-ac2d6aeeeadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52198892,  0.06160485],\n",
       "       [ 0.02403791, -0.22465357],\n",
       "       [ 0.64316296,  0.21995502],\n",
       "       [ 0.35694905, -0.30980858],\n",
       "       [-0.36558658, -0.3628524 ],\n",
       "       [ 0.36707219, -0.1160826 ],\n",
       "       [-0.37629299,  0.2077425 ],\n",
       "       [-0.11914735,  0.50292153],\n",
       "       [-0.51310426,  0.07032475],\n",
       "       [-0.53907985, -0.04915149]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomized PCA\n",
    "# finds an approximation of the first d principal components, it's dramatically faster in some cases\n",
    "\n",
    "rnd_pca = PCA(n_components=2, svd_solver=\"randomized\", random_state=42)\n",
    "X_reduced = rnd_pca.fit_transform(data)\n",
    "X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a16e349-58cb-4588-91b7-c2e162e6b6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52198892,  0.06160485],\n",
       "       [ 0.02403791, -0.22465357],\n",
       "       [ 0.64316296,  0.21995502],\n",
       "       [ 0.35694905, -0.30980858],\n",
       "       [-0.36558658, -0.3628524 ],\n",
       "       [ 0.36707219, -0.1160826 ],\n",
       "       [-0.37629299,  0.2077425 ],\n",
       "       [-0.11914735,  0.50292153],\n",
       "       [-0.51310426,  0.07032475],\n",
       "       [-0.53907985, -0.04915149]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f17bd7-0bf4-43c7-b541-23c39768a84c",
   "metadata": {},
   "source": [
    "## Incremental PCA\n",
    "PCA requires the whole training set to fit in memory. Incremental PCA (IPCA) uses mini-batches and is useful for large training sets and PCA online (i.e, on the fly, as new insatnces arrive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6301f7-1f5e-4ba0-a8e5-6393f67e9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must call the partial_fit() method with each mini-batch,\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faa2c5e-96aa-4a2b-a587-570e5b923c67",
   "metadata": {},
   "source": [
    "Alternatively, you can use NumPy’s memmap class, manipulates a\n",
    "large array stored in a binary file on disk as if it were entirely in memory; the class\n",
    "loads only the data it needs in memory, when it needs it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639bce7d-ed96-4457-abcd-731fc195a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"my_mnist.mmap\"\n",
    "X_mmap = np.memmap(filename, dtype='float32', mode='write', shape=X_train.shape)\n",
    "X_mmap[:] = X_train # could be a loop instead, saving the data chunk by chunk\n",
    "X_mmap.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b788b7d-b2e7-46c2-8c23-fc69830a7469",
   "metadata": {},
   "source": [
    "We can load the memmap file and use it like a regular NumPy array. Since this algorithm uses only\n",
    "a small part of the array at any given time, memory usage remains under control.\n",
    "This makes it possible to call the usual fit() method instead of partial_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac4f46c-2315-4f5c-92f9-6375c1dec193",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mmap = np.memmap(filename, dtype=\"float32\", mode=\"readonly\").reshape(-1, 784)\n",
    "batch_size = X_mmap.shape[0] // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f099a-efb3-4091-99d6-812a9d74f92f",
   "metadata": {},
   "source": [
    "Only the raw binary data is saved to disk --> specify the data type and shape of the array, np.memmap() returns a 1D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b7b5f-4562-4b87-a812-6155f1101200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
